# Fine-Tuned GPT-2 for Domain-Specific Tasks

## ğŸ“Œ Project Overview

This project demonstrates the fine-tuning of GPT-2 using **LoRA (Low-Rank Adaptation)** on a **custom dataset** within a single Jupyter notebook (`.ipynb`). It includes data preparation, fine-tuning processes, and deployment via a Flask API, made publicly accessible through ngrok for interactive querying.

---

## ğŸš€ Key Accomplishments

- âœ… **Fine-tuned GPT-2** using LoRA on a domain-specific dataset.
- âœ… **Deployed an interactive Flask API** accessible via ngrok.
- âœ… **Optimized inference parameters** for concise and coherent responses.

---

## ğŸ› ï¸ Technologies Used

- **GPT-2** (via Hugging Face Transformers)
- **LoRA (PEFT)** for efficient fine-tuning
- **Flask** for API deployment
- **ngrok** for public URL access
- **Google Colab**

---

---

## ğŸ“¸ Screenshot

![Project Screenshot](finetune_llm.png)
